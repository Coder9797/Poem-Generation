{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd40fad-4df8-4636-9578-abfe86e80486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import import_ipynb\n",
    "from Model import GPT\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab297e4-428d-4572-8602-250e23b23b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(77610, 256)\n",
       "  (embeddingpos): Embedding(200, 256)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (att): MultiHeadAttentionLayer(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SelfAttentionLayer(\n",
       "            (wq): Linear(in_features=256, out_features=32, bias=True)\n",
       "            (wk): Linear(in_features=256, out_features=32, bias=True)\n",
       "            (wv): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (wo): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ffl): FeedForwardLayer(\n",
       "        (l1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (l2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (gelu): GELU(approximate='none')\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop1): Dropout(p=0.1, inplace=False)\n",
       "      (drop2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=256, out_features=77610, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT(\n",
    "    vocab_size=77610, \n",
    "    embed_dim=256, \n",
    "    seq_len=200, \n",
    "    dropout=0.1, \n",
    "    n_heads=8, \n",
    "    device=device\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"pretrained_model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcdc17-c139-4c3c-a971-66b71dcf30b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d45afb0-e733-4567-a3f6-be98c6580686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_with_temperature(probabilities, temperature=1.7, top_k=10):\n",
    "    \"\"\"Apply temperature sampling and top-k filtering to add randomness.\"\"\"\n",
    "    probabilities = probabilities / temperature  # Adjust probability distribution\n",
    "    sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)\n",
    "    top_k_probs = sorted_probs[:top_k]\n",
    "    top_k_indices = sorted_indices[:top_k]\n",
    "\n",
    "    # Sample from the top-k tokens\n",
    "    sampled_index = torch.multinomial(F.softmax(top_k_probs, dim=-1), 1).item()\n",
    "    return top_k_indices[sampled_index].item()\n",
    "def generate_text(prompt, max_length=50,temperature=1.7, top_k=10):\n",
    "    # Convert prompt text to token sequence\n",
    "    tokens = tokenizer.texts_to_sequences([prompt])[0]\n",
    "    \n",
    "    # Pad sequence to match model input length\n",
    "    input_ids = pad_sequences([tokens], maxlen=200, padding=\"post\", truncating=\"pre\")\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long).to(device)\n",
    "\n",
    "    # Store generated tokens separately\n",
    "    if not tokens:\n",
    "        print(\"‚ö†Ô∏è Warning: Tokenizer returned an empty sequence. Ensure it was trained on your dataset.\")\n",
    "        return \"\"\n",
    "    generated_tokens = tokens.copy()\n",
    "    print(\"\\nüîπ Initial Input IDs:\", input_ids.tolist())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(200-len(tokens)):\n",
    "            output = model(input_ids)  \n",
    "            probabilities = torch.nn.functional.softmax(output[:, -1, :], dim=-1).squeeze()\n",
    "            #print(f\"Shape of probabilities: {probabilities.shape}\")  \n",
    "            next_token_id = sample_with_temperature(probabilities, temperature=temperature,top_k= top_k)\n",
    "\n",
    "            \n",
    "            generated_tokens.append(next_token_id)  # Append to generated sequence\n",
    "            \n",
    "            # Update input_ids by shifting left and adding the new token\n",
    "            input_ids[0,step+len(tokens)]=next_token_id\n",
    "            #input_ids = torch.cat(\n",
    "                #[input_ids[:, 1:], torch.tensor([[next_token_id]], device=device)], dim=-1\n",
    "            #)\n",
    "            #print(f\"\\nüîπ Iteration {step+1}: Next Token ID = {next_token_id}\")\n",
    "            #print(\"   Updated Input IDs:\", input_ids.tolist())\n",
    "            # Stop if EOS token is generated\n",
    "            if next_token_id == tokenizer.word_index.get(\"<eos>\", None):  # Modify based on your dataset\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.sequences_to_texts([generated_tokens])[0]\n",
    "    \n",
    "    print(\"\\nüîπ Generated Text:\\n\", generated_text)  # Ensure text is printed\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Example Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d21599c-f17f-48d5-95e1-04c44425a398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Initial Input IDs: [[162, 117, 5, 66, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "üîπ Generated Text:\n",
      " once upon a time for to for that to a but to and in i the and for but a the and a for but the for to but i for to as the that and in to in to for to and i i and for i but the but for that a and and in i i in i the a as a and in to for a as as i a the as in as in and that to a that in i but that but i but but i as the the i a and i i in for in i to as as in a for as for in as a and as but as the in but in to but as to and to but and as in the that i in the but that and as and the and the a to to but the a in a a and a i to in as a to to in for the but for a to as for for to for the i and for the that that as a the in but for and the in to in in as the the that\n",
      "\n",
      "‚úÖ Final Output:\n",
      " once upon a time for to for that to a but to and in i the and for but a the and a for but the for to but i for to as the that and in to in to for to and i i and for i but the but for that a and and in i i in i the a as a and in to for a as as i a the as in as in and that to a that in i but that but i but but i as the the i a and i i in for in i to as as in a for as for in as a and as but as the in but in to but as to and to but and as in the that i in the but that and as and the and the a to to but the a in a a and a i to in as a to to in for the but for a to as for for to for the i and for the that that as a the in but for and the in to in in as the the that\n"
     ]
    }
   ],
   "source": [
    "generated_output = generate_text(\"Once upon a time\")\n",
    "print(\"\\n‚úÖ Final Output:\\n\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a390a0ef-eca9-48aa-9455-fb76751a9385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Initial Input IDs: [[7, 37, 236, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "üîπ Generated Text:\n",
      " i love water but but and a but the for the that the for as in in in i i to that as that and for and to as a i a a and i but and a to as as the as the but i as in to i to to a for and the to a the i but for for i to a but as as as but in the that but i i a that the as a a the i as in and for but but in but i that i but i that a for that but a and a but as and and a as the the the to but as but and the as for in to a as a but and a and that in as but to that for as as the and i and that a and a a i and a but in as in a the that and in in in the to that i as as but to a but as and but and to that in to to the to to i in i and a that for but but as that in in that\n",
      "\n",
      "‚úÖ Final Output:\n",
      " i love water but but and a but the for the that the for as in in in i i to that as that and for and to as a i a a and i but and a to as as the as the but i as in to i to to a for and the to a the i but for for i to a but as as as but in the that but i i a that the as a a the i as in and for but but in but i that i but i that a for that but a and a but as and and a as the the the to but as but and the as for in to a as a but and a and that in as but to that for as as the and i and that a and a a i and a but in as in a the that and in in in the to that i as as but to a but as and but and to that in to to the to to i in i and a that for but but as that in in that\n"
     ]
    }
   ],
   "source": [
    "generated_output = generate_text(\"I love water\")\n",
    "print(\"\\n‚úÖ Final Output:\\n\", generated_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
